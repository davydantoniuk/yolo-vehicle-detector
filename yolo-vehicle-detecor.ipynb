{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection Using YOLOv8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on real-time vehicle detection using YOLOv8, a state-of-the-art object detection model. The KITTI dataset ([source](https://www.kaggle.com/datasets/klemenko/kitti-dataset)) was used for training and evaluation, providing high-quality annotated images of vehicles in diverse driving environments. The goal is to accurately detect and classify vehicles in images and videos, enabling applications in traffic monitoring, autonomous driving, and surveillance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np # type: ignore\n",
    "from shutil import copyfile\n",
    "from collections import defaultdict\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly download the dataset from **kagglehub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/kitti-dataset\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"klemenko/kitti-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reorganized the **KITTI dataset** by filtering out unnecessary data and structuring it for YOLOv8 training. First, I defined the paths for images and labels, then created directories for training and validation sets. The dataset was shuffled and split (80% training, 20% validation). Finally, images and corresponding label files were moved into their respective folders to ensure a clean and structured dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset organized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "dataset_path = path \n",
    "images_src = os.path.join(dataset_path, \"data_object_image_2/training/image_2\")  \n",
    "labels_src = os.path.join(dataset_path, \"data_object_label_2/training/label_2\") \n",
    "\n",
    "# Define new dataset structure\n",
    "output_dir = \"data\"\n",
    "train_images_dir = os.path.join(output_dir, \"train/images\")\n",
    "train_labels_dir = os.path.join(output_dir, \"train/labels\")\n",
    "val_images_dir = os.path.join(output_dir, \"val/images\")\n",
    "val_labels_dir = os.path.join(output_dir, \"val/labels\")\n",
    "\n",
    "# Create necessary directories\n",
    "for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Get a list of image files\n",
    "image_files = [f for f in os.listdir(images_src) if f.endswith(\".png\")]\n",
    "image_files.sort()  # Ensure consistent order\n",
    "\n",
    "# Shuffle and split data\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(image_files)\n",
    "\n",
    "split_idx = int(len(image_files) * 0.8)  # 80% train, 20% val\n",
    "train_files = image_files[:split_idx]\n",
    "val_files = image_files[split_idx:]\n",
    "\n",
    "# Function to move files\n",
    "def move_files(file_list, src_folder, dest_folder, ext):\n",
    "    for file in file_list:\n",
    "        file_name = os.path.splitext(file)[0]  # Remove .png extension\n",
    "        src_file = os.path.join(src_folder, file_name + ext)\n",
    "        dest_file = os.path.join(dest_folder, file_name + ext)\n",
    "        if os.path.exists(src_file):  # Move only if file exists\n",
    "            shutil.copy(src_file, dest_file)\n",
    "\n",
    "# Move train images and labels\n",
    "move_files(train_files, images_src, train_images_dir, \".png\")\n",
    "move_files(train_files, labels_src, train_labels_dir, \".txt\")\n",
    "\n",
    "# Move validation images and labels\n",
    "move_files(val_files, images_src, val_images_dir, \".png\")\n",
    "move_files(val_files, labels_src, val_labels_dir, \".txt\")\n",
    "\n",
    "print(\"‚úÖ Dataset organized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the dataset was correctly organized, I checked the number of images and labels in each directory. The script verified the existence of the data folder, displayed its structure, and counted the files in train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ The 'data' folder exists in the working directory!\n",
      "\n",
      "üìÇ Contents of 'data': ['train', 'val']\n",
      "\n",
      "üìÇ Contents of 'train': ['images', 'labels']\n",
      "\n",
      "üìÇ Contents of 'val': ['images', 'labels']\n",
      "üì∏ Number of train images: 5984\n",
      "üìù Number of train labels: 5984\n",
      "üì∏ Number of val images: 1497\n",
      "üìù Number of val labels: 1497\n"
     ]
    }
   ],
   "source": [
    "# Define the main dataset folder\n",
    "data_dir = \"data\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "# Check if \"data\" folder exists\n",
    "if os.path.exists(data_dir) and os.path.isdir(data_dir):\n",
    "    print(\"‚úÖ The 'data' folder exists in the working directory!\")\n",
    "    \n",
    "    # List contents of \"data\" folder\n",
    "    print(\"\\nüìÇ Contents of 'data':\", os.listdir(data_dir))\n",
    "    \n",
    "    # Check contents inside \"train\" and \"val\" folders\n",
    "    if os.path.exists(train_dir):\n",
    "        print(\"\\nüìÇ Contents of 'train':\", os.listdir(train_dir))\n",
    "    else:\n",
    "        print(\"\\n‚ùå 'train' folder not found inside 'data'.\")\n",
    "\n",
    "    if os.path.exists(val_dir):\n",
    "        print(\"\\nüìÇ Contents of 'val':\", os.listdir(val_dir))\n",
    "    else:\n",
    "        print(\"\\n‚ùå 'val' folder not found inside 'data'.\")\n",
    "\n",
    "    # Count number of files in train/images and train/labels\n",
    "    train_images_dir = os.path.join(train_dir, \"images\")\n",
    "    train_labels_dir = os.path.join(train_dir, \"labels\")\n",
    "\n",
    "    if os.path.exists(train_images_dir):\n",
    "        num_train_images = len([f for f in os.listdir(train_images_dir) if os.path.isfile(os.path.join(train_images_dir, f))])\n",
    "        print(f\"üì∏ Number of train images: {num_train_images}\")\n",
    "    else:\n",
    "        print(\"‚ùå Train images folder not found.\")\n",
    "\n",
    "    if os.path.exists(train_labels_dir):\n",
    "        num_train_labels = len([f for f in os.listdir(train_labels_dir) if os.path.isfile(os.path.join(train_labels_dir, f))])\n",
    "        print(f\"üìù Number of train labels: {num_train_labels}\")\n",
    "    else:\n",
    "        print(\"‚ùå Train labels folder not found.\")\n",
    "\n",
    "    # Count number of files in val/images and val/labels\n",
    "    val_images_dir = os.path.join(val_dir, \"images\")\n",
    "    val_labels_dir = os.path.join(val_dir, \"labels\")\n",
    "\n",
    "    if os.path.exists(val_images_dir):\n",
    "        num_val_images = len([f for f in os.listdir(val_images_dir) if os.path.isfile(os.path.join(val_images_dir, f))])\n",
    "        print(f\"üì∏ Number of val images: {num_val_images}\")\n",
    "    else:\n",
    "        print(\"‚ùå Val images folder not found.\")\n",
    "\n",
    "    if os.path.exists(val_labels_dir):\n",
    "        num_val_labels = len([f for f in os.listdir(val_labels_dir) if os.path.isfile(os.path.join(val_labels_dir, f))])\n",
    "        print(f\"üìù Number of val labels: {num_val_labels}\")\n",
    "    else:\n",
    "        print(\"‚ùå Val labels folder not found.\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå The 'data' folder is NOT in the working directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was successfully prepared for training. The train folder contains 5,984 images and labels, while the val folder contains 1,497 images and labels, confirming a correct 80/20 split. Everything is set up properly for model training on **Kaggle**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Analysis and Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Class Distribution Analysis & YOLO Label Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure balanced training, I analyzed the dataset to check how many images contain each class. By converting KITTI labels to YOLO format, I extracted class-wise image counts. This helps identify class imbalances, where some categories, like Car (6684 images), are overrepresented, while others, like Person_sitting (99 images), are underrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KITTI labels successfully converted to YOLO format!\n",
      "\n",
      "üìå Found classes and assigned YOLO class IDs:\n",
      "  - Car ‚Üí 0\n",
      "  - Cyclist ‚Üí 1\n",
      "  - Misc ‚Üí 2\n",
      "  - Pedestrian ‚Üí 3\n",
      "  - Person_sitting ‚Üí 4\n",
      "  - Tram ‚Üí 5\n",
      "  - Truck ‚Üí 6\n",
      "  - Van ‚Üí 7\n",
      "\n",
      "üìä Number of images containing each class:\n",
      "  - Car: 6684 images\n",
      "  - Van: 2145 images\n",
      "  - Truck: 1036 images\n",
      "  - Pedestrian: 1779 images\n",
      "  - Tram: 349 images\n",
      "  - Cyclist: 1141 images\n",
      "  - Person_sitting: 99 images\n",
      "  - Misc: 778 images\n"
     ]
    }
   ],
   "source": [
    "# Function to find all unique class names\n",
    "def find_unique_classes(labels_path):\n",
    "    unique_classes = set()\n",
    "    \n",
    "    for split in [\"train\", \"val\"]:\n",
    "        labels_dir = os.path.join(labels_path, split, \"labels\")\n",
    "        \n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            \n",
    "            with open(label_path, \"r\") as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                class_name = line.strip().split()[0]  # Get class name\n",
    "                if class_name != \"DontCare\":  # Ignore \"DontCare\" objects\n",
    "                    unique_classes.add(class_name)\n",
    "    \n",
    "    return sorted(list(unique_classes))  # Sort for consistency\n",
    "\n",
    "# Step 1: Find all unique classes\n",
    "unique_classes = find_unique_classes(dataset_path)\n",
    "\n",
    "# Step 2: Assign a unique class ID for YOLO\n",
    "class_mapping = {name: idx for idx, name in enumerate(unique_classes)}\n",
    "\n",
    "# Dictionary to track the number of images containing each class\n",
    "class_image_count = defaultdict(int)\n",
    "\n",
    "# Function to convert KITTI labels to YOLO format\n",
    "def convert_kitti_to_yolo(label_path, image_width=1242, image_height=375):\n",
    "    yolo_labels = []\n",
    "    image_classes = set()  # Track unique classes in this image\n",
    "    \n",
    "    with open(label_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        \n",
    "        class_name = parts[0]\n",
    "        if class_name == \"DontCare\":\n",
    "            continue  # Skip DontCare objects\n",
    "        \n",
    "        class_id = class_mapping.get(class_name, None)\n",
    "        if class_id is None:\n",
    "            continue  # Ignore unlisted classes\n",
    "        \n",
    "        # Track that this class appears in the image\n",
    "        image_classes.add(class_name)\n",
    "\n",
    "        # Extract bounding box (xmin, ymin, xmax, ymax)\n",
    "        xmin, ymin, xmax, ymax = map(float, parts[4:8])\n",
    "\n",
    "        # Convert to YOLO format (normalized)\n",
    "        x_center = ((xmin + xmax) / 2) / image_width\n",
    "        y_center = ((ymin + ymax) / 2) / image_height\n",
    "        width = (xmax - xmin) / image_width\n",
    "        height = (ymax - ymin) / image_height\n",
    "\n",
    "        # Store in YOLO format\n",
    "        yolo_labels.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "    # Increment class image count for each unique class in the image\n",
    "    for class_name in image_classes:\n",
    "        class_image_count[class_name] += 1\n",
    "\n",
    "    return yolo_labels\n",
    "\n",
    "# Step 3: Process all label files in train and val folders\n",
    "for split in [\"train\", \"val\"]:\n",
    "    labels_dir = os.path.join(dataset_path, split, \"labels\")\n",
    "    \n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        label_path = os.path.join(labels_dir, label_file)\n",
    "        \n",
    "        # Convert labels\n",
    "        yolo_labels = convert_kitti_to_yolo(label_path)\n",
    "        \n",
    "        # Save back in YOLO format\n",
    "        with open(label_path, \"w\") as file:\n",
    "            file.write(\"\\n\".join(yolo_labels))\n",
    "\n",
    "# Step 4: Output the class mappings and image counts\n",
    "print(\"‚úÖ KITTI labels successfully converted to YOLO format!\")\n",
    "print(\"\\nüìå Found classes and assigned YOLO class IDs:\")\n",
    "for class_name, class_id in class_mapping.items():\n",
    "    print(f\"  - {class_name} ‚Üí {class_id}\")\n",
    "\n",
    "print(\"\\nüìä Number of images containing each class:\")\n",
    "for class_name, count in class_image_count.items():\n",
    "    print(f\"  - {class_name}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is highly imbalanced, which could impact model performance. To address this, I will stabilize class distributions using:\n",
    "\n",
    "- Downsampling Cars to reduce **overrepresentation**.\n",
    "- Oversampling minority classes with augmentations.\n",
    "- Generating synthetic Person_sitting instances using GANs to improve representation.\n",
    "\n",
    "This will create a more balanced dataset for better detection accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Downsampling Overrepresented Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To balance the dataset, I downsampled the Car class, which had an excessive number of instances. I randomly selected 3,500 car instances and removed the excess while keeping other classes unchanged. The filtered images and labels were saved to a new balanced dataset folder, ensuring a more even class distribution.\n",
    "\n",
    "This step helps prevent the model from being biased toward Cars, improving detection accuracy across all classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_path = \"/kaggle/working/data/train\"\n",
    "images_dir = os.path.join(train_path, \"images\")\n",
    "labels_dir = os.path.join(train_path, \"labels\")\n",
    "\n",
    "balanced_path = \"/kaggle/working/data/balanced\"\n",
    "os.makedirs(balanced_path, exist_ok=True)\n",
    "balanced_images = os.path.join(balanced_path, \"images\")\n",
    "balanced_labels = os.path.join(balanced_path, \"labels\")\n",
    "os.makedirs(balanced_images, exist_ok=True)\n",
    "os.makedirs(balanced_labels, exist_ok=True)\n",
    "\n",
    "# Collect all Car instances\n",
    "car_instances = []\n",
    "for label_file in os.listdir(labels_dir):\n",
    "    label_path = os.path.join(labels_dir, label_file)\n",
    "    if not os.path.isfile(label_path):\n",
    "        continue  # Skip directories\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('0 '):\n",
    "                car_instances.append((label_file, line.strip()))\n",
    "\n",
    "# Select 4000 instances\n",
    "random.seed(42)\n",
    "selected_cars = random.sample(car_instances, 3500)\n",
    "\n",
    "# Group selected cars by their label file\n",
    "selected_by_file = defaultdict(list)\n",
    "for label_file, car_line in selected_cars:\n",
    "    selected_by_file[label_file].append(car_line)\n",
    "\n",
    "# Process each image\n",
    "for label_file in os.listdir(labels_dir):\n",
    "    label_path = os.path.join(labels_dir, label_file)\n",
    "    if not os.path.isfile(label_path):\n",
    "        continue\n",
    "    \n",
    "    # Check for both .jpg and .png extensions\n",
    "    base_name = os.path.splitext(label_file)[0]\n",
    "    for ext in ['.jpg', '.jpeg', '.png']:\n",
    "        image_file = base_name + ext\n",
    "        src_image = os.path.join(images_dir, image_file)\n",
    "        if os.path.exists(src_image):\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Warning: No image found for {label_file}\")\n",
    "        continue\n",
    "\n",
    "    # Read original labels\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    \n",
    "    # Filter selected Cars and keep others\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith('0 '):\n",
    "            if line in selected_by_file.get(label_file, []):\n",
    "                new_lines.append(line)\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "    \n",
    "    # Save if not empty\n",
    "    if new_lines:\n",
    "        copyfile(src_image, os.path.join(balanced_images, image_file))\n",
    "        with open(os.path.join(balanced_labels, label_file), 'w') as f:\n",
    "            f.write('\\n'.join(new_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Oversampling Minority Classes with Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To balance the dataset, I applied data augmentation to increase the number of samples for underrepresented classes (Cyclist, Misc, Tram, Truck, and Van). Using transformations like flipping, rotation, brightness adjustment, and padding, I generated additional images while ensuring valid bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: Starting with 1321 samples, needing 1679 more\n",
      "Class 1: Generated 1679/1679 valid augmentations\n",
      "Class 2: Starting with 778 samples, needing 2222 more\n",
      "Class 2: Generated 2222/2222 valid augmentations\n",
      "Class 5: Starting with 412 samples, needing 2588 more\n",
      "Class 5: Generated 2588/2588 valid augmentations\n",
      "Class 6: Starting with 896 samples, needing 2104 more\n",
      "Class 6: Generated 2104/2104 valid augmentations\n",
      "Class 7: Starting with 2351 samples, needing 649 more\n",
      "Class 7: Generated 649/649 valid augmentations\n"
     ]
    }
   ],
   "source": [
    "# Enhanced augmentation pipeline with safety constraints\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.PadIfNeeded(min_height=416, min_width=416, p=1.0)\n",
    "], bbox_params=A.BboxParams(\n",
    "    format='yolo',\n",
    "    label_fields=['class_ids'],\n",
    "    min_area=0.001,  # Discard boxes smaller than 0.1% of image area\n",
    "    min_visibility=0.1  # Discard boxes with <10% visibility\n",
    "))\n",
    "\n",
    "def validate_bbox(bbox):\n",
    "    \"\"\"Clamp bbox coordinates to [0,1] with precision handling\"\"\"\n",
    "    return [\n",
    "        max(0.0, min(1.0, round(float(bbox[0]), 6))),\n",
    "        max(0.0, min(1.0, round(float(bbox[1]), 6))),\n",
    "        max(0.0, min(1.0, round(float(bbox[2]), 6))),\n",
    "        max(0.0, min(1.0, round(float(bbox[3]), 6)))\n",
    "    ]\n",
    "\n",
    "classes_to_oversample = {1: 3000, 2: 3000, 5: 3000, 6: 3000, 7: 3000}\n",
    "\n",
    "for class_id, target in classes_to_oversample.items():\n",
    "    # Collect instances with enhanced validation\n",
    "    instances = []\n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        label_path = os.path.join(labels_dir, label_file)\n",
    "        if not os.path.isfile(label_path):\n",
    "            continue\n",
    "        \n",
    "        # Find corresponding image with extension check\n",
    "        base_name = os.path.splitext(label_file)[0]\n",
    "        image_path = None\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            test_path = os.path.join(images_dir, f\"{base_name}{ext}\")\n",
    "            if os.path.exists(test_path):\n",
    "                image_path = test_path\n",
    "                break\n",
    "        if not image_path:\n",
    "            continue\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue  # Skip invalid lines\n",
    "                \n",
    "                try:\n",
    "                    if int(parts[0]) == class_id:\n",
    "                        validated_bbox = validate_bbox(parts[1:5])\n",
    "                        instances.append((label_file, parts, image_path))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    needed = max(target - len(instances), 0)\n",
    "    print(f\"Class {class_id}: Starting with {len(instances)} samples, needing {needed} more\")\n",
    "    \n",
    "    # Augmentation with retry logic\n",
    "    success_count = 0\n",
    "    attempt = 0\n",
    "    while success_count < needed and attempt < needed * 3:  # Prevent infinite loops\n",
    "        attempt += 1\n",
    "        \n",
    "        # Select random instance\n",
    "        label_file, parts, image_path = random.choice(instances)\n",
    "        \n",
    "        # Load and validate image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Validate and prepare bbox\n",
    "        try:\n",
    "            bbox = validate_bbox(parts[1:5])\n",
    "            if sum(bbox[2:]) < 0.01:  # Skip tiny boxes\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Apply augmentation\n",
    "        try:\n",
    "            transformed = aug(image=image, bboxes=[bbox], class_ids=[class_id])\n",
    "            aug_image = transformed['image']\n",
    "            aug_bboxes = transformed['bboxes']\n",
    "            \n",
    "            if len(aug_bboxes) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Final validation\n",
    "            aug_bbox = validate_bbox(aug_bboxes[0])\n",
    "            if (aug_bbox[0] >= 0 and aug_bbox[1] >= 0 and\n",
    "                (aug_bbox[0] + aug_bbox[2]) <= 1.0 and\n",
    "                (aug_bbox[1] + aug_bbox[3]) <= 1.0):\n",
    "                \n",
    "                # Save successful augmentation\n",
    "                aug_image_name = f\"aug_{class_id}_{success_count}.jpg\"\n",
    "                aug_label_name = f\"aug_{class_id}_{success_count}.txt\"\n",
    "                cv2.imwrite(os.path.join(balanced_images, aug_image_name),\n",
    "                          cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "                with open(os.path.join(balanced_labels, aug_label_name), 'w') as f:\n",
    "                    f.write(f\"{class_id} {' '.join(map(str, aug_bbox))}\")\n",
    "                success_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    print(f\"Class {class_id}: Generated {success_count}/{needed} valid augmentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augmentation pipeline successfully increased the dataset size for minority classes, improving balance and helping the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure data quality, I performed a sanity check on the dataset by verifying that all label files contain valid YOLO-formatted bounding boxes. The script checked for:\n",
    "\n",
    "- Missing or incorrectly formatted bounding boxes.\n",
    "- Bounding boxes outside the valid range (0 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid labels: \n"
     ]
    }
   ],
   "source": [
    "def verify_dataset(labels_dir, images_dir):\n",
    "    bad_files = []\n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        label_path = os.path.join(labels_dir, label_file)\n",
    "        with open(label_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    bad_files.append(f\"{label_file}: line {i+1}\")\n",
    "                    continue\n",
    "                try:\n",
    "                    bbox = list(map(float, parts[1:5]))\n",
    "                    if any(not (0 <= x <= 1) for x in bbox):\n",
    "                        bad_files.append(f\"{label_file}: invalid bbox {bbox}\")\n",
    "                except:\n",
    "                    bad_files.append(f\"{label_file}: non-float values\")\n",
    "    \n",
    "    print(\"Invalid labels:\", \"\\n\".join(bad_files))\n",
    "\n",
    "verify_dataset(labels_dir, images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors were found, confirming that the dataset is clean and ready for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Generating Synthetic Person_sitting Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Person_sitting class had only 99 images, making it severely underrepresented. To improve detection accuracy, I generated 1,500 synthetic instances by:\n",
    "\n",
    "- Extracting Person_sitting patches from existing images.\n",
    "- Applying augmentation (rotation, brightness contrast) to create variations.\n",
    "- Pasting patches onto random background images to simulate realistic scenes.\n",
    "- Recalculating bounding boxes to maintain YOLO format integrity.\n",
    "\n",
    "This process increased the dataset diversity and ensured better model performance for this rare class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Person_sitting instances with image verification\n",
    "person_instances = []\n",
    "for label_file in os.listdir(labels_dir):\n",
    "    label_path = os.path.join(labels_dir, label_file)\n",
    "    if not os.path.isfile(label_path):\n",
    "        continue\n",
    "    \n",
    "    # Find corresponding image\n",
    "    base_name = os.path.splitext(label_file)[0]\n",
    "    image_path = None\n",
    "    for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
    "        test_path = os.path.join(images_dir, base_name + ext)\n",
    "        if os.path.exists(test_path):\n",
    "            image_path = test_path\n",
    "            break\n",
    "    \n",
    "    if not image_path:\n",
    "        continue\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if int(parts[0]) == 4:\n",
    "                person_instances.append((label_file, parts, image_path))\n",
    "\n",
    "needed = 1500 - len(person_instances)\n",
    "\n",
    "# Get list of valid background images\n",
    "bg_images = []\n",
    "for img_file in os.listdir(images_dir):\n",
    "    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        bg_images.append(os.path.join(images_dir, img_file))\n",
    "\n",
    "# Augmentation for patches\n",
    "patch_aug = A.Compose([\n",
    "    A.Rotate(limit=45, p=1, border_mode=cv2.BORDER_CONSTANT),\n",
    "    A.RandomBrightnessContrast(p=0.8)\n",
    "])\n",
    "\n",
    "for i in range(needed):\n",
    "    try:\n",
    "        # Select random instance\n",
    "        label_file, parts, image_path = random.choice(person_instances)\n",
    "        \n",
    "        # Load source image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "            \n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        # Extract bbox with boundary checks\n",
    "        x_center, y_center, bw, bh = map(float, parts[1:5])\n",
    "        x1 = max(0, int((x_center - bw/2) * w))\n",
    "        y1 = max(0, int((y_center - bh/2) * h))\n",
    "        x2 = min(w, int((x_center + bw/2) * w))\n",
    "        y2 = min(h, int((y_center + bh/2) * h))\n",
    "        \n",
    "        # Skip invalid boxes\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "            \n",
    "        patch = image[y1:y2, x1:x2]\n",
    "        if patch.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Augment patch\n",
    "        augmented = patch_aug(image=patch)\n",
    "        patch_augmented = augmented['image']\n",
    "        \n",
    "        # Skip if augmentation destroyed the patch\n",
    "        if patch_augmented.size == 0:\n",
    "            continue\n",
    "            \n",
    "        ph, pw = patch_augmented.shape[:2]\n",
    "\n",
    "        # Select valid background\n",
    "        bg_path = random.choice(bg_images)\n",
    "        bg_image = cv2.imread(bg_path)\n",
    "        if bg_image is None:\n",
    "            continue\n",
    "            \n",
    "        bh, bw = bg_image.shape[:2]\n",
    "\n",
    "        # Ensure patch fits in background\n",
    "        if pw >= bw or ph >= bh:\n",
    "            continue  # Skip or resize patch here if desired\n",
    "            \n",
    "        x = np.random.randint(0, bw - pw)\n",
    "        y = np.random.randint(0, bh - ph)\n",
    "\n",
    "        # Paste patch\n",
    "        bg_image[y:y+ph, x:x+pw] = patch_augmented\n",
    "\n",
    "        # Calculate new bbox\n",
    "        new_x_center = (x + pw/2) / bw\n",
    "        new_y_center = (y + ph/2) / bh\n",
    "        new_w = pw / bw\n",
    "        new_h = ph / bh\n",
    "\n",
    "        # Save\n",
    "        synth_image_name = f\"synth_4_{i}.jpg\"\n",
    "        synth_label_name = f\"synth_4_{i}.txt\"\n",
    "        cv2.imwrite(os.path.join(balanced_images, synth_image_name), bg_image)\n",
    "        with open(os.path.join(balanced_labels, synth_label_name), 'w') as f:\n",
    "            f.write(f\"4 {new_x_center:.6f} {new_y_center:.6f} {new_w:.6f} {new_h:.6f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating synthetic sample {i}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Checking the Final Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After balancing the dataset, I verified the number of instances for each class to ensure proper distribution. This step confirms that the downsampling, oversampling, and synthetic data generation worked as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KITTI labels successfully converted to YOLO format!\n",
      "\n",
      "üìå Found classes and assigned YOLO class IDs:\n",
      "  - Car ‚Üí 0 (3500 instances in train set)\n",
      "  - Cyclist ‚Üí 1 (3000 instances in train set)\n",
      "  - Misc ‚Üí 2 (3000 instances in train set)\n",
      "  - Pedestrian ‚Üí 3 (3591 instances in train set)\n",
      "  - Person_sitting ‚Üí 4 (1500 instances in train set)\n",
      "  - Tram ‚Üí 5 (3000 instances in train set)\n",
      "  - Truck ‚Üí 6 (3000 instances in train set)\n",
      "  - Van ‚Üí 7 (3000 instances in train set)\n"
     ]
    }
   ],
   "source": [
    "# Define paths to label directories\n",
    "train_labels_dir = \"data/balanced/labels\"\n",
    "val_labels_dir = \"data/val/labels\"\n",
    "\n",
    "# Correct YOLO class mapping\n",
    "class_mapping = {\n",
    "    \"Car\": 0,\n",
    "    \"Cyclist\": 1,\n",
    "    \"Misc\": 2,\n",
    "    \"Pedestrian\": 3,\n",
    "    \"Person_sitting\": 4,\n",
    "    \"Tram\": 5,\n",
    "    \"Truck\": 6,\n",
    "    \"Van\": 7\n",
    "}\n",
    "\n",
    "# Function to count occurrences of each class in YOLO labels\n",
    "def get_class_counts(label_dir):\n",
    "    class_counts = {id_: 0 for id_ in class_mapping.values()}  # Initialize counts\n",
    "\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "        with open(label_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 0:\n",
    "                class_id = int(parts[0])  # Extract class ID\n",
    "                \n",
    "                # Count occurrences\n",
    "                if class_id in class_counts:\n",
    "                    class_counts[class_id] += 1\n",
    "                else:\n",
    "                    class_counts[class_id] = 1\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "# Function to get unique classes\n",
    "def get_unique_classes(label_dir):\n",
    "    unique_classes = set()\n",
    "\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "        with open(label_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 0:\n",
    "                class_id = int(parts[0])  # Extract class ID\n",
    "                unique_classes.add(class_id)\n",
    "\n",
    "    return unique_classes\n",
    "\n",
    "# Get unique class IDs from both train and val datasets\n",
    "train_classes = get_unique_classes(train_labels_dir)\n",
    "val_classes = get_unique_classes(val_labels_dir)\n",
    "\n",
    "# Combine unique classes from both sets\n",
    "all_unique_classes = train_classes.union(val_classes)\n",
    "\n",
    "# Get class occurrences in the train set\n",
    "train_class_counts = get_class_counts(train_labels_dir)\n",
    "\n",
    "# Reverse mapping: Convert YOLO class IDs back to names\n",
    "reverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "# Output results\n",
    "print(\"‚úÖ KITTI labels successfully converted to YOLO format!\\n\")\n",
    "print(\"üìå Found classes and assigned YOLO class IDs:\")\n",
    "\n",
    "for class_id in sorted(all_unique_classes):\n",
    "    class_name = reverse_class_mapping.get(class_id, f\"Unknown_{class_id}\")\n",
    "    count = train_class_counts.get(class_id, 0)\n",
    "    print(f\"  - {class_name} ‚Üí {class_id} ({count} instances in train set)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now well-balanced, ensuring that all classes have a similar number of instances. This will help the YOLOv8 model generalize better and improve detection performance across all categories."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
